# Adam Optimizer: Implementation and Performance Analysis

A comprehensive implementation and analysis of the Adam (Adaptive Moment Estimation) optimizer, developed as part of MATH 303: Linear and Non-Linear Programming coursework at Zewail City. This project compares a custom Adam implementation with Keras' built-in version across different neural network architectures.

## ğŸ“Š Project Overview

This project provides both theoretical analysis and practical implementation of the Adam optimizer, demonstrating its effectiveness compared to traditional optimization methods through experimentation on the MNIST dataset.

## ğŸ¯ Key Objectives

- Implement Adam optimizer from scratch, following the  mathematical definitions
- Compare custom implementation with Keras' built-in Adam optimizer
- Analyze performance across different neural network architectures
- Evaluate convergence characteristics, training efficiency, and accuracy
- Validate theoretical properties through empirical experiments

## ğŸ“ Project Structure

```
â”œâ”€â”€ notebook.ipynb                # Main implementation and analysis
â”œâ”€â”€ README.md                     # Project documentation
â””â”€â”€ report.pdf                       # Academic report and documentation
```

## ğŸ§® Mathematical Foundation

### Adam Update Rules
The Adam optimizer updates parameters using adaptive moment estimation:

```
mt = Î²1 * mt-1 + (1 - Î²1) * gt
vt = Î²2 * vt-1 + (1 - Î²2) * gtÂ²
mÌ‚t = mt / (1 - Î²1^t)
vÌ‚t = vt / (1 - Î²2^t)
Î¸t+1 = Î¸t - Î· * mÌ‚t / (âˆšvÌ‚t + Îµ)
```

Where:
- `Î·`: Learning rate
- `Î²1, Î²2`: Decay rates for moment estimates
- `Îµ`: Small constant for numerical stability
- `gt`: Gradient at time step t

## ğŸ—ï¸ Implementation Details

### Custom Adam Optimizer Classes

1. **Standalone Implementation**: Pure NumPy implementation for educational purposes
2. **Keras-Compatible Implementation**: Custom optimizer class inheriting from `tf.keras.optimizers.Optimizer`

### Model Architectures Tested

**Simple Neural Network (SimpleNN)**:
- Flatten layer (28Ã—28Ã—1 â†’ 784)
- Dense layer (128 units, ReLU)
- Dense layer (64 units, ReLU)
- Output layer (10 units, softmax)

**Convolutional Neural Network (CNN)**:
- Conv2D (32 filters, 3Ã—3, ReLU)
- MaxPooling2D (2Ã—2)
- Conv2D (64 filters, 3Ã—3, ReLU)
- MaxPooling2D (2Ã—2)
- Flatten + Dense (128 units, ReLU)
- Output layer (10 units, softmax)

## ğŸ› ï¸ Technologies Used

- **Python 3.10**
- **TensorFlow/Keras** - Deep learning framework
- **NumPy** - Numerical computations
- **Matplotlib** - Data visualization
- **Seaborn** - Statistical plotting
- **Google Colab** - Development environment

## ğŸ“ˆ Key Results

### Performance Comparison

| Model & Optimizer | Training Time (s) | Test Accuracy (%) | Final Loss |
|-------------------|-------------------|-------------------|------------|
| SimpleNN (Keras Adam) | 26.7 | 97.65 | 0.0503 |
| SimpleNN (Custom Adam) | 27.1 | 97.53 | 0.0511 |
| CNN (Keras Adam) | 345.2 | **98.94** | **0.0195** |
| CNN (Custom Adam) | 348.5 | 98.88 | 0.0199 |

### Key Findings

âœ… **Custom Implementation Validation**: Our custom Adam optimizer achieved performance nearly identical to Keras' implementation (difference < 3%)

âœ… **Architecture Impact**: CNN models achieved significantly higher accuracy (98.94% vs 97.65%) but required 13x longer training time

âœ… **Convergence Stability**: Both Adam implementations showed superior convergence characteristics with stable validation metrics

âœ… **Training Efficiency**: SimpleNN models are ideal for applications prioritizing speed over maximum accuracy

## ğŸš€ Getting Started

### Prerequisites
```bash
pip install tensorflow numpy matplotlib seaborn jupyter
```

### Running the Analysis
1. Clone this repository
2. Open `notebook.ipynb` in Jupyter or Google Colab
3. Run all cells sequentially to reproduce results

## ğŸ“Š Experimental Setup

### Dataset
- **MNIST**: 60,000 training images, 10,000 test images
- **Preprocessing**: Normalized to [0,1], reshaped to (28,28,1)
- **Labels**: One-hot encoded for 10-digit classes

### Hyperparameters
- Learning rate: 0.001
- Î²1: 0.9 (first moment decay)
- Î²2: 0.999 (second moment decay)
- Îµ: 1e-8 (numerical stability)
- Batch size: 64
- Epochs: 5

## ğŸ“ˆ Visualizations

The project generates comprehensive visualizations including:
- Training vs validation loss curves
- Accuracy progression over epochs
- Performance comparison bar charts
- Convergence analysis plots

## ğŸ” Analysis Highlights

### Convergence Behavior
- CNN architectures achieved significantly lower final loss (0.0195-0.0199) vs SimpleNN (0.0503-0.0511)
- Both implementations showed similar convergence patterns
- CNN models demonstrated more stable validation metrics

### Implementation Validation
- Custom Adam matched Keras Adam performance (loss difference < 0.8% for CNN, < 3% for SimpleNN)
- Training times were comparable between custom and built-in implementations
- Mathematical correctness validated through empirical results

## ğŸ”® Future Work

- **Dataset Expansion**: Test on CIFAR-10, ImageNet for complex scenarios
- **Hyperparameter Optimization**: Systematic exploration of learning rates and decay factors
- **Scalability Testing**: Apply to larger models (ResNet, Transformers)
- **Hybrid Approaches**: Investigate Adam combinations with other optimization techniques
- **Regularization Impact**: Study interaction with dropout, batch normalization

## ğŸ“ Academic Context

**Course**: MATH 303 - Linear and Non-Linear Programming  
**Institution**: Zewail City  
**Authors**: Mazen Ahmed Basha, Ahmed Sameh Morsy, Mahmoud Abdelrahman, Haneen Alaa, Jilan Ismail  
**Supervision**: Prof. Ahmed Abdelsamea, TA: Hossam Fathy, Youssef Mohamed

## ğŸ“š References

1. Kingma, D. P., & Ba, J. (2015). Adam: A Method for Stochastic Optimization. ICLR.
2. Duchi, J., Hazan, E., & Singer, Y. (2011). Adaptive subgradient methods for online learning and stochastic optimization. JMLR.
3. Tieleman, T., & Hinton, G. (2012). RMSProp: Divide the gradient by a running average of its recent magnitude. Coursera.

## ğŸ“„ License

This project is created for educational purposes as part of coursework requirements.

---

*This project demonstrates the mathematical foundations and practical implementation of adaptive optimization algorithms, showcasing skills in deep learning, mathematical modeling, and empirical analysis.*
